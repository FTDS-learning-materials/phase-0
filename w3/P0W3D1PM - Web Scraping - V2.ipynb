{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Web Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lhIDkp7qX2R"
      },
      "source": [
        "## Setup\n",
        "Web scraping is a process of extracting data from website. This process is very important to know as a data scientist since sometimes we cannot get data easily as we querying the data from the database or download Kaggle. \n",
        "\n",
        "For this session, we are going to scrape a website using BeautifulSoup and Selenium. To install these packages, you can run the command below.\n",
        "```\n",
        "pip install bs4 selenium\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### For Safari\n",
        "- To use Selenium with Safari, first we need to enable Remote Automation feature in Safari.\n",
        "    1. Open Safari settings/preferences and go to **Advanced** tab.\n",
        "    <img src=\"https://github.com/FTDS-learning-materials/phase-0/blob/main/img/web-scraping-1.png?raw=true\" />\n",
        "    2. Check the \"Show features for web developers\" option.\n",
        "    <img src=\"https://github.com/FTDS-learning-materials/phase-0/blob/main/img/web-scraping-2.png?raw=true\" />\n",
        "    3. After that, click on the **Developer** tab and check the \"Allow Remote Automation\" option.\n",
        "    <img src=\"https://github.com/FTDS-learning-materials/phase-0/blob/main/img/web-scraping-3.png?raw=true\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0aMoG9gqX2S"
      },
      "source": [
        "## Basic Web Component\n",
        "\n",
        "The website that you are scraping in this lesson contains several components. Those are:\n",
        "- HTML — the main content of the page.\n",
        "- CSS — used to add styling to make the page look nicer.\n",
        "- JS — Javascript files add interactivity to web pages.\n",
        "- Images — image formats, such as JPG and PNG, allow web pages to show pictures.\n",
        "\n",
        "There’s a lot that happens behind the scenes to render a page nicely, but we don’t need to worry about most of it when we’re web scraping. When we perform web scraping, we’re interested in the main content of the web page, so we look primarily at the HTML. \n",
        "\n",
        "Hence, we must first learn the fundamentals of how HTML works. But don't worry, we don't need to dive in deeply into it.\n",
        "\n",
        "### HTML Structure\n",
        "\n",
        "HyperText Markup Language (HTML) is the standard markup language for creating Web pages which consists of series of elements.\n",
        "\n",
        "HTML has many functions that are similar to what you might find in a word processor like Microsoft Word — it can make text bold, create paragraphs, and so on.\n",
        "\n",
        "<img src=\"https://developer.mozilla.org/en-US/docs/Glossary/Element/anatomy-of-an-html-element.png\"/><br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR5EdxCbqX2T"
      },
      "source": [
        "## Accessing the Web\n",
        "\n",
        "Now, we will access https://www.scrapethissite.com/pages/forms/ for this lesson.\n",
        "\n",
        "<img src=\"https://www.scrapingbee.com/blog/getting-started-with-mechanicalsoup/hockey-teams-page_hu2277696443619028977.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To start scraping for data we can setup our code like below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JtNvku54qX2V",
        "outputId": "ac5db257-05ec-481b-ca6d-7946f98b6aa2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<html lang=\"en\">\\n <head>\\n  <meta charset=\"utf-8\"/>\\n  <title>\\n   Hockey Teams: Forms, Searching and Pagination | Scrape This Site | A public sandbox for learning web scraping\\n  </title>\\n  <link href=\"/static/images/scraper-icon.png\" rel=\"icon\" type=\"image/png\"/>\\n  <meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\\n  <meta content=\"Browse through a database of NHL team stats since 1990. Practice building a scraper that handles common website interface components.\" name=\"description\"/>\\n  <link crossorigin=\"anonymous\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css\" integrity=\"sha256-MfvZlkHCEqatNoGiOXveE8FIwMzZg4W85qfrfIFBfYc= sha512-dTfge/zgoMYp'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# import packages\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# initialize selenium browser - Chrome\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "# define target url\n",
        "url = \"https://www.scrapethissite.com/pages/forms\"\n",
        "\n",
        "# tell the browser to open the web page\n",
        "driver.get(url)\n",
        "\n",
        "# extracting html from the page - in strings\n",
        "html = driver.page_source\n",
        "\n",
        "# convert html string to BeautifulSoup object\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "# close the browser session\n",
        "driver.quit()\n",
        "\n",
        "# display object as output - limit to 700 characters\n",
        "display(soup.prettify()[:700])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see from the output, we successfully retrieve the HTML data from the targeted page. But this data is still raw, so next we must define which data that we want to extract from the page. For example, we want to extract team names from the page. To do this, first we have to identify which element will be targeted using Inspect Element feature in our browser.\n",
        "\n",
        "<img src=\"https://github.com/FTDS-learning-materials/phase-0/blob/main/img/web-scraping-4.png?raw=true\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From inspect element, we know that each team name is located in `<td>` element with attribute `class=\"name\"`. Next we can start extracting the targeted elements using `find()` or `find_all()` method.\n",
        "\n",
        "- `find()` -> will return 1 element.\n",
        "- `find_all()` -> will return 1 or many elements.\n",
        "\n",
        "Both of these methods have 2 parameters where first parameter is **required** for the element name, and second one is **optional** for the element attribute(s).\n",
        "\n",
        "Example\n",
        "\n",
        "- `find(\"td\", {\"class\": \"name\"})`\n",
        "- `find_all(\"td\", {\"class\": \"name\"})`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "result:  <td class=\"name\">\n",
            "                            Boston Bruins\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            Buffalo Sabres\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            Calgary Flames\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            Chicago Blackhawks\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            Detroit Red Wings\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            Edmonton Oilers\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            Hartford Whalers\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            Los Angeles Kings\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            Minnesota North Stars\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            Montreal Canadiens\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            New Jersey Devils\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            New York Islanders\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            New York Rangers\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            Philadelphia Flyers\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            Pittsburgh Penguins\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            Quebec Nordiques\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            St. Louis Blues\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            Toronto Maple Leafs\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            Vancouver Canucks\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            Washington Capitals\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            Winnipeg Jets\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            Boston Bruins\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            Buffalo Sabres\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            Calgary Flames\n",
            "                        </td>\n",
            "result:  <td class=\"name\">\n",
            "                            Chicago Blackhawks\n",
            "                        </td>\n"
          ]
        }
      ],
      "source": [
        "# import packages\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# initialize selenium browser - Chrome\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "# define target url\n",
        "url = \"https://www.scrapethissite.com/pages/forms\"\n",
        "\n",
        "# tell the browser to open the web page\n",
        "driver.get(url)\n",
        "\n",
        "# extracting html from the page - in strings\n",
        "html = driver.page_source\n",
        "\n",
        "# convert html string to BeautifulSoup object\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "# find td elements\n",
        "result = soup.find_all(\"td\", {\"class\": \"name\"})\n",
        "\n",
        "# because the result is in list, we can use loop to process each elements\n",
        "for element in result:\n",
        "    print(\"result: \", element)\n",
        "\n",
        "# close the browser session\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although we have successfully get each `td` elements, but this data is not clean yet. Now we have to extract the content using `get_text()` from each element."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "result:  Boston Bruins\n",
            "result:  Buffalo Sabres\n",
            "result:  Calgary Flames\n",
            "result:  Chicago Blackhawks\n",
            "result:  Detroit Red Wings\n",
            "result:  Edmonton Oilers\n",
            "result:  Hartford Whalers\n",
            "result:  Los Angeles Kings\n",
            "result:  Minnesota North Stars\n",
            "result:  Montreal Canadiens\n",
            "result:  New Jersey Devils\n",
            "result:  New York Islanders\n",
            "result:  New York Rangers\n",
            "result:  Philadelphia Flyers\n",
            "result:  Pittsburgh Penguins\n",
            "result:  Quebec Nordiques\n",
            "result:  St. Louis Blues\n",
            "result:  Toronto Maple Leafs\n",
            "result:  Vancouver Canucks\n",
            "result:  Washington Capitals\n",
            "result:  Winnipeg Jets\n",
            "result:  Boston Bruins\n",
            "result:  Buffalo Sabres\n",
            "result:  Calgary Flames\n",
            "result:  Chicago Blackhawks\n"
          ]
        }
      ],
      "source": [
        "# import packages\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# initialize selenium browser - Chrome\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "# define target url\n",
        "url = \"https://www.scrapethissite.com/pages/forms\"\n",
        "\n",
        "# tell the browser to open the web page\n",
        "driver.get(url)\n",
        "\n",
        "# extracting html from the page - in strings\n",
        "html = driver.page_source\n",
        "\n",
        "# convert html string to BeautifulSoup object\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "# find td elements\n",
        "result = soup.find_all(\"td\", {\"class\": \"name\"})\n",
        "\n",
        "for element in result:\n",
        "    # process each element\n",
        "    # extract the content using get_text()\n",
        "    # add strip() to remove whitespace from the extracted content\n",
        "    print(\"result: \", element.get_text().strip())\n",
        "\n",
        "# close the browser session\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we sucessfully extract each team names from the page. Next we can store this result to DataFrame for analysis purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Team</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Boston Bruins</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Buffalo Sabres</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Calgary Flames</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Chicago Blackhawks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Detroit Red Wings</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Edmonton Oilers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Hartford Whalers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Los Angeles Kings</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Minnesota North Stars</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Montreal Canadiens</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>New Jersey Devils</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>New York Islanders</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>New York Rangers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Philadelphia Flyers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Pittsburgh Penguins</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Quebec Nordiques</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>St. Louis Blues</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Toronto Maple Leafs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Vancouver Canucks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Washington Capitals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Winnipeg Jets</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Boston Bruins</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Buffalo Sabres</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Calgary Flames</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Chicago Blackhawks</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Team\n",
              "0           Boston Bruins\n",
              "1          Buffalo Sabres\n",
              "2          Calgary Flames\n",
              "3      Chicago Blackhawks\n",
              "4       Detroit Red Wings\n",
              "5         Edmonton Oilers\n",
              "6        Hartford Whalers\n",
              "7       Los Angeles Kings\n",
              "8   Minnesota North Stars\n",
              "9      Montreal Canadiens\n",
              "10      New Jersey Devils\n",
              "11     New York Islanders\n",
              "12       New York Rangers\n",
              "13    Philadelphia Flyers\n",
              "14    Pittsburgh Penguins\n",
              "15       Quebec Nordiques\n",
              "16        St. Louis Blues\n",
              "17    Toronto Maple Leafs\n",
              "18      Vancouver Canucks\n",
              "19    Washington Capitals\n",
              "20          Winnipeg Jets\n",
              "21          Boston Bruins\n",
              "22         Buffalo Sabres\n",
              "23         Calgary Flames\n",
              "24     Chicago Blackhawks"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# import packages\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# initialize DataFrame\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# prepare temp list for storing all team names\n",
        "teamNames = []\n",
        "\n",
        "# initialize selenium browser - Chrome\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "# define target url\n",
        "url = \"https://www.scrapethissite.com/pages/forms\"\n",
        "\n",
        "# tell the browser to open the web page\n",
        "driver.get(url)\n",
        "\n",
        "# extracting html from the page - in strings\n",
        "html = driver.page_source\n",
        "\n",
        "# convert html string to BeautifulSoup object\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "# find td elements\n",
        "result = soup.find_all(\"td\", {\"class\": \"name\"})\n",
        "\n",
        "for element in result:\n",
        "    # add validation before append to list\n",
        "    # check if the element exists\n",
        "    if element != None:\n",
        "        teamNames.append(element.get_text().strip())\n",
        "    else:\n",
        "        # else add None to list\n",
        "        teamNames.append(None)\n",
        "\n",
        "# close the browser session\n",
        "driver.quit()\n",
        "\n",
        "# populate the DataFrame\n",
        "df['Team'] = teamNames\n",
        "\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And that's it, we have done simple web scraping from a sandbox website. After this you can use this technique to any use case that you may found in the nearest future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## More Samples\n",
        "### Extracting from Multiple Pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Team</th>\n",
              "      <th>Year</th>\n",
              "      <th>Wins</th>\n",
              "      <th>Losses</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Boston Bruins</td>\n",
              "      <td>1990</td>\n",
              "      <td>44</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Buffalo Sabres</td>\n",
              "      <td>1990</td>\n",
              "      <td>31</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Calgary Flames</td>\n",
              "      <td>1990</td>\n",
              "      <td>46</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Chicago Blackhawks</td>\n",
              "      <td>1990</td>\n",
              "      <td>49</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Detroit Red Wings</td>\n",
              "      <td>1990</td>\n",
              "      <td>34</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>Boston Bruins</td>\n",
              "      <td>1995</td>\n",
              "      <td>40</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>Buffalo Sabres</td>\n",
              "      <td>1995</td>\n",
              "      <td>33</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>Calgary Flames</td>\n",
              "      <td>1995</td>\n",
              "      <td>34</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>Chicago Blackhawks</td>\n",
              "      <td>1995</td>\n",
              "      <td>40</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>Colorado Avalanche</td>\n",
              "      <td>1995</td>\n",
              "      <td>47</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>125 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Team  Year Wins Losses\n",
              "0         Boston Bruins  1990   44     24\n",
              "1        Buffalo Sabres  1990   31     30\n",
              "2        Calgary Flames  1990   46     26\n",
              "3    Chicago Blackhawks  1990   49     23\n",
              "4     Detroit Red Wings  1990   34     38\n",
              "..                  ...   ...  ...    ...\n",
              "120       Boston Bruins  1995   40     31\n",
              "121      Buffalo Sabres  1995   33     42\n",
              "122      Calgary Flames  1995   34     37\n",
              "123  Chicago Blackhawks  1995   40     28\n",
              "124  Colorado Avalanche  1995   47     25\n",
              "\n",
              "[125 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# import packages\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# initialize DataFrame\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# prepare temp list for storing all datas\n",
        "teamNames = []\n",
        "teamYears = []\n",
        "teamWins = []\n",
        "teamLosses = []\n",
        "\n",
        "# initialize selenium browser - Chrome\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "\n",
        "# using for loop to iterate multiple pages from the website\n",
        "for page in range(1, 6):\n",
        "    # define target url\n",
        "    url = f\"https://www.scrapethissite.com/pages/forms?page_num={page}\"\n",
        "\n",
        "    # tell the browser to open the web page\n",
        "    driver.get(url)\n",
        "\n",
        "    # extracting html from the page - in strings\n",
        "    html = driver.page_source\n",
        "\n",
        "    # convert html string to BeautifulSoup object\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "    # find tr elements with class team\n",
        "    result = soup.find_all(\"tr\", {\"class\": \"team\"})\n",
        "\n",
        "    for tr_element in result:\n",
        "        # find every td element inside of tr\n",
        "        name = tr_element.find('td', {\"class\": \"name\"})\n",
        "        year = tr_element.find('td', {\"class\": \"year\"})\n",
        "        win = tr_element.find('td', {\"class\": \"wins\"})\n",
        "        loss = tr_element.find('td', {\"class\": \"losses\"})\n",
        "\n",
        "        # extract each td element inside tr\n",
        "        # don't forget to validate if each element exists before extracting with get_text()\n",
        "        if name != None:\n",
        "            teamNames.append(name.get_text().strip())\n",
        "        else:\n",
        "            teamNames.append(None)\n",
        "\n",
        "        if year != None:\n",
        "            teamYears.append(year.get_text().strip())\n",
        "        else:\n",
        "            teamYears.append(None)\n",
        "\n",
        "        if win != None:\n",
        "            teamWins.append(win.get_text().strip())\n",
        "        else:\n",
        "            teamWins.append(None)\n",
        "\n",
        "        if loss != None:\n",
        "            teamLosses.append(loss.get_text().strip())\n",
        "        else:\n",
        "            teamLosses.append(None)\n",
        "\n",
        "# close the browser session\n",
        "driver.quit()\n",
        "\n",
        "# populate the DataFrame\n",
        "df['Team'] = teamNames\n",
        "df['Year'] = teamYears\n",
        "df['Wins'] = teamWins\n",
        "df['Losses'] = teamLosses\n",
        "\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Notes**\n",
        "- In some cases you may need to add `time.sleep()` when performing scraping, this will make `BeautifulSoup` wait for the pages finished loading its data.\n",
        "    ```\n",
        "    ...\n",
        "    import time --> import `time` package first\n",
        "    ...\n",
        "    driver.get(url)\n",
        "\n",
        "    time.sleep(5) --> add `time.sleep()` in between opening the web page and before extracting html from the page\n",
        "\n",
        "    html = driver.page_source\n",
        "    ...\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSCNbYWDqX2X"
      },
      "source": [
        "### Accessing Each Individual Detail \n",
        "\n",
        "<img src=\"https://github.com/FTDS-learning-materials/phase-0/blob/main/img/web-scraping-5.png?raw=true\">\n",
        "\n",
        "In this example, we're going to use Gramedia website. In this [page](https://www.gramedia.com/categories/buku/komik) we can see that there are many subcategories (\"Fantasi\", \"Fiksi Sejarah\", \"Horror\", etc.). Now our target is to scrape every book information inside each subcategory. So, we will access the individual page and scrape some information about the book `title`, `author`, and `price`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hh32wnYgqX2Y",
        "outputId": "fdf5a0a5-09fb-428b-8314-b9df2bec845d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Koloni : Sakti Family Begins</td>\n",
              "      <td>Alisnaik</td>\n",
              "      <td>Rp45.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Koloni Gundala: Amuk Vol. 2</td>\n",
              "      <td>Iskandar Salim, Wahyu Widiatmoko, Wastukancono...</td>\n",
              "      <td>Rp51.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Light Novel: Overlord 3 - The Bloody Valkyrie</td>\n",
              "      <td>Kugane Maruyama</td>\n",
              "      <td>Rp135.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Light Novel The Rising Shield Hero 02</td>\n",
              "      <td>Aneko Yusagi</td>\n",
              "      <td>Rp115.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Light Novel So I'm a Spider, So What? 2</td>\n",
              "      <td>Okina Baba</td>\n",
              "      <td>Rp115.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>Koloni Jawara Sejati</td>\n",
              "      <td>Tanfidz Tammamudin &amp; Ragha Sukma</td>\n",
              "      <td>Rp36.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>Eknath</td>\n",
              "      <td>Sotogakpaketomat</td>\n",
              "      <td>Rp89.100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>Komik Jingga dalam Elegi</td>\n",
              "      <td>Esti Kinasih</td>\n",
              "      <td>Rp62.250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>Flesh Out</td>\n",
              "      <td>Bella Zmr</td>\n",
              "      <td>Rp33.750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>Your Name.</td>\n",
              "      <td>Makoto Shinkai</td>\n",
              "      <td>Rp98.100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>121 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Title  \\\n",
              "0                     Koloni : Sakti Family Begins   \n",
              "1                      Koloni Gundala: Amuk Vol. 2   \n",
              "2    Light Novel: Overlord 3 - The Bloody Valkyrie   \n",
              "3            Light Novel The Rising Shield Hero 02   \n",
              "4          Light Novel So I'm a Spider, So What? 2   \n",
              "..                                             ...   \n",
              "116                           Koloni Jawara Sejati   \n",
              "117                                         Eknath   \n",
              "118                       Komik Jingga dalam Elegi   \n",
              "119                                      Flesh Out   \n",
              "120                                     Your Name.   \n",
              "\n",
              "                                                Author      Price  \n",
              "0                                             Alisnaik   Rp45.000  \n",
              "1    Iskandar Salim, Wahyu Widiatmoko, Wastukancono...   Rp51.000  \n",
              "2                                      Kugane Maruyama  Rp135.000  \n",
              "3                                         Aneko Yusagi  Rp115.000  \n",
              "4                                           Okina Baba  Rp115.000  \n",
              "..                                                 ...        ...  \n",
              "116                   Tanfidz Tammamudin & Ragha Sukma   Rp36.000  \n",
              "117                                   Sotogakpaketomat   Rp89.100  \n",
              "118                                       Esti Kinasih   Rp62.250  \n",
              "119                                          Bella Zmr   Rp33.750  \n",
              "120                                     Makoto Shinkai   Rp98.100  \n",
              "\n",
              "[121 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# import packages\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# initialize DataFrame\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# prepare list variables\n",
        "bookTitles = []\n",
        "bookAuthors = []\n",
        "bookPrices = []\n",
        "\n",
        "# open browser\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "# open web page\n",
        "url = \"https://www.gramedia.com/categories/buku/komik\"\n",
        "driver.get(url)\n",
        "\n",
        "# add delay for 3 seconds to wait the page to finished loading.\n",
        "time.sleep(3)\n",
        "\n",
        "# extract HTML from the page\n",
        "html = driver.page_source\n",
        "page = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "# iterate every subcategories\n",
        "for subcategory in page.find_all('a', {\"data-sentry-component\": \"CategoryPill\"}):\n",
        "    # open every subcategory page\n",
        "    driver.get(subcategory['href'].lower())\n",
        "\n",
        "    # add delay for 5 seconds to wait the page to finished loading.\n",
        "    time.sleep(5)\n",
        "\n",
        "    # extract HTML from the page\n",
        "    html = driver.page_source\n",
        "    sub_page = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "    # iterate every books inside subcategory page\n",
        "    for books in sub_page.find_all('div', {\"data-testid\": \"productCardContent\"}):\n",
        "        # find each element\n",
        "        title = books.find('h2', {\"data-testid\": \"productCardTitle\"})\n",
        "        author = books.find('div', {\"data-testid\": \"productCardAuthor\"})\n",
        "        price = books.find('div', {\"data-testid\": \"productCardFinalPrice\"})\n",
        "\n",
        "        # extract each element\n",
        "        if title != None:\n",
        "            bookTitles.append(title.get_text().strip())\n",
        "        else:\n",
        "            bookTitles.append(None)\n",
        "\n",
        "        if author != None:\n",
        "            bookAuthors.append(author.get_text().strip())\n",
        "        else:\n",
        "            bookAuthors.append(None)\n",
        "\n",
        "        if price != None:\n",
        "            bookPrices.append(price.get_text().strip())\n",
        "        else:\n",
        "            bookPrices.append(None)\n",
        "\n",
        "driver.quit()\n",
        "\n",
        "# populate DataFrame\n",
        "df['Title'] = bookTitles\n",
        "df['Author'] = bookAuthors\n",
        "df['Price'] = bookPrices\n",
        "\n",
        "display(df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
