{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Window Function**\n",
        "\n",
        "We will use dc_bikeshare_q1_2012 table again that we used in day-3 lecture. However, if you already removed the table, you can run these query that stored in a sql script. You can access the script [here](https://github.com/FTDS-learning-materials/phase-0/blob/main/src/w2d3pm.sql).\n",
        "\n",
        "A window function performs a calculation across a set of table rows that are somehow related to the current row. This is comparable to the type of calculation that can be done with an aggregate function. But unlike regular aggregate functions, use of a window function does not cause rows to become grouped into a single output row — the rows retain their separate identities. Behind the scenes, the window function is able to access more than just the current row of the query result."
      ],
      "metadata": {
        "id": "-Pseof9M4rld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most practical example of this is a running total:\n",
        "\n",
        "```sql\n",
        "SELECT duration_seconds,\n",
        "       SUM(duration_seconds) OVER (ORDER BY start_time) AS running_total\n",
        "  FROM dc_bikeshare_q1_2012\n",
        "```\n",
        "\n",
        "When using window functions, you can apply the same aggregates that you would under normal circumstances—SUM, COUNT, and AVG. The easiest way to understand these is to re-run the previous example with some additional functions. Make\n",
        "\n",
        "```sql\n",
        "SELECT start_terminal,\n",
        "       duration_seconds,\n",
        "       SUM(duration_seconds) OVER\n",
        "         (PARTITION BY start_terminal) AS running_total,\n",
        "       COUNT(duration_seconds) OVER\n",
        "         (PARTITION BY start_terminal) AS running_count,\n",
        "       AVG(duration_seconds) OVER\n",
        "         (PARTITION BY start_terminal) AS running_avg\n",
        "  FROM dc_bikeshare_q1_2012\n",
        " WHERE start_time < '2012-01-08'\n",
        "```"
      ],
      "metadata": {
        "id": "VkgPYPiy5iz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROW_NUMBER()\n",
        "\n",
        "ROW_NUMBER() does just what it sounds like—displays the number of a given row. It starts are 1 and numbers the rows according to the ORDER BY part of the window statement. ROW_NUMBER() does not require you to specify a variable within the parentheses:\n",
        "\n",
        "```sql\n",
        "SELECT start_terminal,\n",
        "       start_time,\n",
        "       duration_seconds,\n",
        "       ROW_NUMBER() OVER (ORDER BY start_time)\n",
        "                    AS rn\n",
        "  FROM dc_bikeshare_q1_2012\n",
        " WHERE start_time < '2012-01-08'\n",
        "```\n",
        "\n",
        "Using the PARTITION BY clause will allow you to begin counting 1 again in each partition. The following query starts the count over again for each terminal:\n",
        "\n",
        "```sql\n",
        "SELECT start_terminal,\n",
        "       start_time,\n",
        "       duration_seconds,\n",
        "       ROW_NUMBER() OVER (PARTITION BY start_terminal\n",
        "                          ORDER BY start_time)\n",
        "                    AS rn\n",
        "  FROM dc_bikeshare_q1_2012\n",
        " WHERE start_time < '2012-01-08'\n",
        "```"
      ],
      "metadata": {
        "id": "hbIWmlA35juJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RANK() and DENSE_RANK()\n",
        "\n",
        "RANK() is slightly different from ROW_NUMBER(). If you order by start_time, for example, it might be the case that some terminals have rides with two identical start times. In this case, they are given the same rank, whereas ROW_NUMBER() gives them different numbers. In the following query, you notice the 4th and 5th observations for start_terminal 31000—they are both given a rank of 4, and the following result receives a rank of 6:\n",
        "\n",
        "```sql\n",
        "SELECT start_terminal,\n",
        "       duration_seconds,\n",
        "       RANK() OVER (PARTITION BY start_terminal\n",
        "                    ORDER BY start_time)\n",
        "              AS ranking\n",
        "  FROM dc_bikeshare_q1_2012\n",
        " WHERE start_time < '2012-01-08'\n",
        "```\n",
        "\n",
        "You can also use DENSE_RANK() instead of RANK() depending on your application. Imagine a situation in which three entries have the same value. Using either command, they will all get the same rank. For the sake of this example, let's say it's \"2.\" Here's how the two commands would evaluate the next results differently:\n",
        "\n",
        "- RANK() would give the identical rows a rank of 2, then skip ranks 3 and 4, so the next result would be 5\n",
        "- DENSE_RANK() would still give all the identical rows a rank of 2, but the following row would be 3—no ranks would be skipped.\n",
        "\n",
        "You can use window functions to identify what percentile (or quartile, or any other subdivision) a given row falls into. The syntax is NTILE(*# of buckets*). In this case, ORDER BY determines which column to use to determine the quartiles (or whatever number of 'tiles you specify). For example:\n",
        "\n",
        "```sql\n",
        "SELECT start_terminal,\n",
        "       duration_seconds,\n",
        "       NTILE(4) OVER\n",
        "         (PARTITION BY start_terminal ORDER BY duration_seconds)\n",
        "          AS quartile,\n",
        "       NTILE(5) OVER\n",
        "         (PARTITION BY start_terminal ORDER BY duration_seconds)\n",
        "         AS quintile,\n",
        "       NTILE(100) OVER\n",
        "         (PARTITION BY start_terminal ORDER BY duration_seconds)\n",
        "         AS percentile\n",
        "  FROM dc_bikeshare_q1_2012\n",
        " WHERE start_time < '2012-01-08'\n",
        " ORDER BY start_terminal, duration_seconds\n",
        "```"
      ],
      "metadata": {
        "id": "zCgQM8c955U-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LAG and LEAD\n",
        "\n",
        "It can often be useful to compare rows to preceding or following rows, especially if you've got the data in an order that makes sense. You can use LAG or LEAD to create columns that pull values from other rows—all you need to do is enter which column to pull from and how many rows away you'd like to do the pull. LAG pulls from previous rows and LEAD pulls from following rows:\n",
        "\n",
        "```sql\n",
        "SELECT start_terminal,\n",
        "       duration_seconds,\n",
        "       LAG(duration_seconds, 1) OVER\n",
        "         (PARTITION BY start_terminal ORDER BY duration_seconds) AS lag,\n",
        "       LEAD(duration_seconds, 1) OVER\n",
        "         (PARTITION BY start_terminal ORDER BY duration_seconds) AS lead\n",
        "  FROM dc_bikeshare_q1_2012\n",
        " WHERE start_time < '2012-01-08'\n",
        " ORDER BY start_terminal, duration_seconds\n",
        "```\n",
        "\n",
        "This is especially useful if you want to calculate differences between rows:\n",
        "\n",
        "```sql\n",
        "SELECT start_terminal,\n",
        "       duration_seconds,\n",
        "       duration_seconds -LAG(duration_seconds, 1) OVER\n",
        "         (PARTITION BY start_terminal ORDER BY duration_seconds)\n",
        "         AS difference\n",
        "  FROM dc_bikeshare_q1_2012\n",
        " WHERE start_time < '2012-01-08'\n",
        " ORDER BY start_terminal, duration_seconds\n",
        "```\n",
        "\n",
        "The first row of the difference column is null because there is no previous row from which to pull. Similarly, using LEAD will create nulls at the end of the dataset. If you'd like to make the results a bit cleaner, you can wrap it in an outer query to remove nulls:\n",
        "\n",
        "```sql\n",
        "SELECT *\n",
        "  FROM (\n",
        "    SELECT start_terminal,\n",
        "           duration_seconds,\n",
        "           duration_seconds -LAG(duration_seconds, 1) OVER\n",
        "             (PARTITION BY start_terminal ORDER BY duration_seconds)\n",
        "             AS difference\n",
        "      FROM dc_bikeshare_q1_2012\n",
        "     WHERE start_time < '2012-01-08'\n",
        "     ORDER BY start_terminal, duration_seconds\n",
        "       ) sub\n",
        " WHERE sub.difference IS NOT NULL\n",
        " ```"
      ],
      "metadata": {
        "id": "YtywMSFu6FUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NTILE\n",
        "\n",
        "If you're planning to write several window functions in to the same query, using the same window, you can create an alias. Take the NTILE example above:\n",
        "\n",
        "```sql\n",
        "SELECT start_terminal,\n",
        "       duration_seconds,\n",
        "       NTILE(4) OVER\n",
        "         (PARTITION BY start_terminal ORDER BY duration_seconds)\n",
        "         AS quartile,\n",
        "       NTILE(5) OVER\n",
        "         (PARTITION BY start_terminal ORDER BY duration_seconds)\n",
        "         AS quintile,\n",
        "       NTILE(100) OVER\n",
        "         (PARTITION BY start_terminal ORDER BY duration_seconds)\n",
        "         AS percentile\n",
        "  FROM dc_bikeshare_q1_2012\n",
        " WHERE start_time < '2012-01-08'\n",
        " ORDER BY start_terminal, duration_seconds\n",
        " ```"
      ],
      "metadata": {
        "id": "mbazRIRl6MzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pivot Data in SQL"
      ],
      "metadata": {
        "id": "qC3FtbmE7y4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SETUP\n",
        "\n",
        "Download dataset used in this lecture from here: https://github.com/ardhiraka/PFDS_sources/blob/master/players.csv and https://github.com/ardhiraka/PFDS_sources/blob/master/teams.csv.\n",
        "\n",
        "\n",
        "You will make two tables using create and insert the data by csv files.\n",
        "\n",
        "1. First, you may need to run this query to make the tables:\n",
        "\n",
        "```sql\n",
        "BEGIN;\n",
        "\n",
        "CREATE TABLE players (\n",
        "    full_school_name VARCHAR(255),\n",
        "    school_name VARCHAR(255),\n",
        "    player_name VARCHAR(255),\n",
        "    position VARCHAR(255),\n",
        "    height FLOAT,\n",
        "    weight FLOAT,\n",
        "    year VARCHAR(255),\n",
        "    hometown VARCHAR(255),\n",
        "    state VARCHAR(255),\n",
        "    id INT PRIMARY KEY\n",
        ");\n",
        "\n",
        "CREATE TABLE teams (\n",
        "    division VARCHAR(100),\n",
        "    conference VARCHAR(100),\n",
        "    school_name VARCHAR(100),\n",
        "    roster_url VARCHAR(200),\n",
        "    id INT PRIMARY KEY\n",
        ");\n",
        "\n",
        "COMMIT;\n",
        "```\n",
        "\n",
        "2. Then, you can right click on the table name in the database schema and choose `Import/Export Data...`. You may need turn on the Header toggle in the `Options`."
      ],
      "metadata": {
        "id": "O58XLNHn83pY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pivot Table\n",
        "\n",
        "Let's start by aggregating the data to show the number of players of each year in each conference, similar to the first example in the inner join lesson:\n",
        "\n",
        "```sql\n",
        "SELECT teams.conference AS conference,\n",
        "       players.year,\n",
        "       COUNT(1) AS players\n",
        "  FROM players\n",
        "  JOIN teams teams\n",
        "    ON teams.school_name = players.school_name\n",
        " GROUP BY 1,2\n",
        " ORDER BY 1,2\n",
        "```\n",
        "\n",
        "In order to transform the data, we'll need to put the above query into a subquery. It can be helpful to create the subquery and select all columns from it before starting to make transformations. Re-running the query at incremental steps like this makes it easier to debug if your query doesn't run. Note that you can eliminate the ORDER BY clause from the subquery since we'll reorder the results in the outer query.\n",
        "\n",
        "```sql\n",
        "SELECT *\n",
        "  FROM (\n",
        "        SELECT teams.conference AS conference,\n",
        "               players.year,\n",
        "               COUNT(1) AS players\n",
        "            FROM players players\n",
        "\t\t\tJOIN teams\n",
        "            ON teams.school_name = players.school_name\n",
        "         GROUP BY 1,2\n",
        "       ) sub\n",
        "```\n",
        "\n",
        "Assuming that works as planned (results should look exactly the same as the first query), it's time to break the results out into different columns for various years. Each item in the SELECT statement creates a column, so you'll have to create a separate column for each year:\n",
        "\n",
        "```sql\n",
        "SELECT conference,\n",
        "       SUM(CASE WHEN year = 'FR' THEN players ELSE NULL END) AS fr,\n",
        "       SUM(CASE WHEN year = 'SO' THEN players ELSE NULL END) AS so,\n",
        "       SUM(CASE WHEN year = 'JR' THEN players ELSE NULL END) AS jr,\n",
        "       SUM(CASE WHEN year = 'SR' THEN players ELSE NULL END) AS sr\n",
        "  FROM (\n",
        "        SELECT teams.conference AS conference,\n",
        "               players.year,\n",
        "               COUNT(1) AS players\n",
        "          FROM players\n",
        "          JOIN teams\n",
        "            ON teams.school_name = players.school_name\n",
        "         GROUP BY 1,2\n",
        "       ) sub\n",
        " GROUP BY 1\n",
        " ORDER BY 1\n",
        " ```\n",
        "\n",
        "But this could still be made a little better. You'll notice that the above query produces a list that is ordered alphabetically by Conference. It might make more sense to add a \"total players\" column and order by that (largest to smallest):\n",
        "\n",
        "```sql\n",
        "SELECT conference,\n",
        "       SUM(players) AS total_players,\n",
        "       SUM(CASE WHEN year = 'FR' THEN players ELSE NULL END) AS fr,\n",
        "       SUM(CASE WHEN year = 'SO' THEN players ELSE NULL END) AS so,\n",
        "       SUM(CASE WHEN year = 'JR' THEN players ELSE NULL END) AS jr,\n",
        "       SUM(CASE WHEN year = 'SR' THEN players ELSE NULL END) AS sr\n",
        "  FROM (\n",
        "        SELECT teams.conference AS conference,\n",
        "               players.year,\n",
        "               COUNT(1) AS players\n",
        "          FROM players\n",
        "          JOIN teams\n",
        "            ON teams.school_name = players.school_name\n",
        "         GROUP BY 1,2\n",
        "       ) sub\n",
        " GROUP BY 1\n",
        " ORDER BY 2 DESC\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "vb7XszZMC30a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SQL with Python"
      ],
      "metadata": {
        "id": "2_oIHcHTDRPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2"
      ],
      "metadata": {
        "id": "CTn8XA4NDcWJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to PostgreSQL Server (Localhost)\n",
        "\n",
        "To connect with your Postgres server, you can run this syntax:\n",
        "\n",
        "```python\n",
        "# Replace these parameters with your actual database credentials\n",
        "db_user = \"postgres\"\n",
        "db_password = \"1234\" #Use your own password\n",
        "db_host = \"localhost\"  # Usually \"localhost\" if running locally\n",
        "db_port = \"5432\"  # Default is 5432\n",
        "\n",
        "connection = psycopg2.connect(\n",
        "    user=db_user,\n",
        "    password=db_password,\n",
        "    host=db_host,\n",
        "    port=db_port\n",
        ")\n",
        "```\n",
        "\n",
        "Because our case is in local, you cannot perform this section in Google Colab. Using Visual Studio Code or Jupyter Notebook is a must."
      ],
      "metadata": {
        "id": "WKvC_I0bDiF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Cursor Object\n",
        "\n",
        "A cursor allows you to execute SQL queries on the connected database.\n",
        "\n",
        "```py\n",
        "cursor = connection.cursor()\n",
        "```"
      ],
      "metadata": {
        "id": "mrIjynATG0uI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Case 1 - Create a Database\n",
        "\n",
        "```py\n",
        "connection.autocommit = True\n",
        "\n",
        "q = sql.SQL('CREATE DATABASE pysql;')\n",
        "cursor.execute(q)\n",
        "\n",
        "cursor.close()\n",
        "connection.close()\n",
        "```\n",
        "\n",
        "In this code snippet, we are executing a sequence of operations to create a new PostgreSQL database named \"pysql.\"\n",
        "\n",
        "1. `connection.autocommit = True`:\n",
        "   This line sets the connection's `autocommit` property to `True`. By default, when you establish a connection to a PostgreSQL database using `psycopg2`, it operates in \"transaction mode.\" This means that each database operation (like creating a table, inserting data, etc.) is treated as part of a single transaction. The changes are not committed (saved permanently) to the database until you explicitly call `commit()` on the connection or close the connection. Setting `autocommit = True` ensures that each individual SQL statement is automatically committed as soon as it is executed. This is particularly useful when you want to perform standalone operations that do not require transactional integrity.\n",
        "\n",
        "2. `q = sql.SQL('CREATE DATABASE pysql;')`:\n",
        "   In this line, we define an SQL statement using the `sql.SQL` class from the `psycopg2.sql` module. Using `sql.SQL` is recommended to avoid SQL injection vulnerabilities. Here, we are creating an SQL object that represents the SQL statement to create a new database called \"pysql.\"\n",
        "\n",
        "3. `cursor.execute(q)`:\n",
        "   We execute the SQL statement `q` using the cursor's `execute()` method. The cursor is an object that allows us to interact with the database and execute SQL queries. In this case, we use the `execute()` method to create the database \"pysql.\"\n",
        "\n",
        "4. `cursor.close()` and `connection.close()`:\n",
        "   After executing the SQL statement, we close the cursor and the connection to the database. Closing the cursor frees up resources associated with it, and closing the connection releases any connections and resources used to communicate with the PostgreSQL database."
      ],
      "metadata": {
        "id": "NWhJoDTPHS7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Case 2 - Create a Table\n",
        "\n",
        "```py\n",
        "connection = psycopg2.connect(\n",
        "    user=db_user,\n",
        "    password=db_password,\n",
        "    host=db_host,\n",
        "    port=db_port,\n",
        "    database=\"pysql\"\n",
        ")\n",
        "\n",
        "cursor = connection.cursor()\n",
        "\n",
        "q = sql.SQL('''CREATE TABLE IF NOT EXISTS students (\n",
        "    id SERIAL PRIMARY KEY,\n",
        "    name VARCHAR(50),\n",
        "    age INTEGER,\n",
        "    campus_id INTEGER,\n",
        "    total_grade FLOAT\n",
        ");''')\n",
        "\n",
        "cursor.execute(q)\n",
        "connection.commit()\n",
        "```\n",
        "\n",
        "Since we already created a new database named \"pysql\", we should reconnect with the server along with the database.\n",
        "\n",
        "after executing the SQL statement using `cursor.execute(q)`, the `connection.commit()` method is called. This additional step is necessary to commit the changes made to the database explicitly.\n",
        "\n",
        "By default, when you perform database operations using `psycopg2`, the changes are not committed immediately. Instead, they are kept as part of an ongoing transaction until you explicitly call `commit()`."
      ],
      "metadata": {
        "id": "8AdD_7SDHefT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Case 3 - Insert Data into Table\n",
        "\n",
        "```py\n",
        "data_values = [\n",
        "    ('Rafif Iman', 20, 1, 85.5),\n",
        "    ('Hana Arisona', 21, 2, 90.2),\n",
        "    ('Raka Purnomo', 19, 1, 78.9),\n",
        "    ('Danu Irfansyah', 20, 3, 92.7),\n",
        "    ('Rachman Ardhi', 22, 2, 88.1)\n",
        "    ]\n",
        "\n",
        "insert_query = \"INSERT INTO students (name, age, campus_id, total_grade) VALUES (%s,%s,%s,%s)\"\n",
        "\n",
        "cursor.executemany(insert_query, data_values)\n",
        "\n",
        "connection.commit()\n",
        "```\n",
        "\n",
        "The `cursor.executemany()` method in `psycopg2` is used to execute the same SQL statement multiple times with different sets of parameters. It is particularly useful when you want to perform bulk insert or update operations efficiently."
      ],
      "metadata": {
        "id": "2MiIdGW_Hkeo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Case 4 - Performing a SELECT Query\n",
        "\n",
        "Since we only want to retrieve data from the server, in performing DQL you don't need `commit()`.\n",
        "\n",
        "```py\n",
        "select_query = \"SELECT * FROM students;\"\n",
        "cursor.execute(select_query)\n",
        "\n",
        "    # Fetch and print the results\n",
        "rows = cursor.fetchall()\n",
        "for row in rows:\n",
        "    print(row)\n",
        "```\n",
        "\n",
        "To print all data you can use `cursor.fetchall()`. However, if you want to print the first row only, you can use `cursor.fetchone()`.\n",
        "\n",
        "```py\n",
        "select_query = \"SELECT * FROM students;\"\n",
        "cursor.execute(select_query)\n",
        "\n",
        "result = cursor.fetchone()\n",
        "\n",
        "print(result)\n",
        "```"
      ],
      "metadata": {
        "id": "X90sNW6oHoIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to Pandas\n",
        "\n",
        "Perhaps, using fetchall() or fetchone() to retrieve data is inconvinient, especially when we want to manipulate, preprocess, and cleaning the data.\n",
        "\n",
        "Pandas provides function that can directly read the data from DQL:\n",
        "\n",
        "```py\n",
        "select_query = '''SELECT campus_id, MIN(age) AS min_age, MAX(age) AS max_age\n",
        "                    FROM students\n",
        "                    GROUP BY campus_id;'''\n",
        "df = pd.read_sql_query(select_query, connection)\n",
        "\n",
        "df\n",
        "```"
      ],
      "metadata": {
        "id": "91zmkB3pHswC"
      }
    }
  ]
}